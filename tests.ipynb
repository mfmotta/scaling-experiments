{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5//10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dir_path = pathlib.Path().resolve()#+'/results'\n",
    "\n",
    "print(dir_path)\n",
    "#if not dir_path.exists():\n",
    "    #dir_path.parent.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class ComputeLayers:\n",
    "    def __init__(self, varying_layer:int):\n",
    "        '''Initializes ComputeLayers class\n",
    "        :param varying_layer: Position of layer with size to be computed. With 1 corresponding\n",
    "        the first layer, and 2, to the second.'''\n",
    "        self.varying_layer = varying_layer\n",
    "        assert self.varying_layer in [1,2], 'varying_layer value must be 1 or 2'\n",
    "\n",
    "    def layer_values(self, params_number:int, input_size:int, fixed_layer_size:int, output_size:int)->int:\n",
    "        '''\n",
    "        Function to compute varying layer values. Works with only two hidden layers at the moment.\n",
    "        :param params_number: total number of parameters\n",
    "        :param input_size: input size\n",
    "        :param fixed_layer_size: fixed layer size/width\n",
    "        :param output_size: output layer size\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        p, u0, u1, u2, u3 = sym.symbols('p u0 u1 u2 u3')\n",
    "        layers = [u0, u1, u2, u3]\n",
    "\n",
    "        solve_for_layer = sym.solveset((u0*u1 + u1)+ (u1*u2 + u2) + ( u2*u3+ u3) - p, layers[self.varying_layer])\n",
    "        variables = [p]+ layers[:self.varying_layer]+layers[self.varying_layer+1:]\n",
    "        expression = sym.lambdify(variables, solve_for_layer, modules=['math'])\n",
    "        numerical_solution = expression(params_number, input_size, fixed_layer_size, output_size)\n",
    "        layer_size =  int(numerical_solution)\n",
    "        assert layer_size>=1, 'must be >= 1, reduce fixed_layer_size or increase params_number'\n",
    "\n",
    "        layers_expression = sym.lambdify(layers, layers, modules=['math'])\n",
    "\n",
    "        if self.varying_layer==1:\n",
    "            return layers_expression(input_size, layer_size, fixed_layer_size, output_size)\n",
    "        else:\n",
    "            return layers_expression(input_size, fixed_layer_size, layer_size, output_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8100/1079659448.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mclass\u001B[0m \u001B[0mNet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mModule\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlayers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mList\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minitialization\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m         '''\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[1;33m:\u001B[0m\u001B[0mparam\u001B[0m \u001B[0mlayers\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mlist\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfirst\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msecond\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0moutput\u001B[0m \u001B[0mlayer\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, layers:List(int), initialization:str):\n",
    "        '''\n",
    "\n",
    "        :param layers: list with input, first, second, and output layer values\n",
    "        :param initialization: 'xavier' or 'kaiming'\n",
    "        '''\n",
    "\n",
    "        [u0, u1, u2, u3] = layers\n",
    "\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(u0, u1)\n",
    "        if initialization == 'kaiming':\n",
    "            nn.init.kaiming_uniform_(self.fc1.weight, a= np.sqrt(2), mode='fan_in', nonlinearity='leaky_relu')\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.fc1.weight, gain= 1/np.sqrt(3))\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=u1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        self.fc2 = nn.Linear(u1, u2)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=u2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        if initialization == 'kaiming':\n",
    "           nn.init.kaiming_uniform_(self.fc2.weight, a= np.sqrt(2), mode='fan_in', nonlinearity='leaky_relu')\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.fc2.weight, gain= 1/np.sqrt(3))\n",
    "\n",
    "        self.fc3 = nn.Linear(u2, u3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}